{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# pip3 install --user PyMuPDF\n",
    "import fitz\n",
    "\n",
    "date = \"2020-04-11\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_stream(stream):\n",
    "    data_raw = []\n",
    "    data_transformed = []\n",
    "    rotparams = None\n",
    "    npatches = 0\n",
    "    for line in stream.splitlines():\n",
    "        if line.endswith(\" cm\"):\n",
    "            # page 146 of https://www.adobe.com/content/dam/acom/en/devnet/pdf/pdfs/pdf_reference_archives/PDFReference.pdf\n",
    "            rotparams = list(map(float,line.split()[:-1]))\n",
    "        elif line.endswith(\" l\"):\n",
    "            x,y = list(map(float,line.split()[:2]))\n",
    "            a,b,c,d,e,f = rotparams\n",
    "            xp = a*x+c*y+e\n",
    "            yp = b*x+d*y+f\n",
    "            data_transformed.append([xp,yp])\n",
    "            data_raw.append([x,y])\n",
    "        elif line.endswith(\" m\"):\n",
    "            npatches += 1\n",
    "        else:\n",
    "            pass\n",
    "    data_raw = np.array(data_raw)\n",
    "    if len(data_raw) < 1:\n",
    "        return dict(data=np.array(data_raw), npatches=npatches, good=False)\n",
    "    base_x, base_y = data_raw[-1]\n",
    "    good = False\n",
    "    if base_x == 0.:\n",
    "        data_raw[:,1] = base_y - data_raw[:,1]\n",
    "        data_raw[:,1] *= 100/60.\n",
    "        data_raw = data_raw[data_raw[:,1]!=0.]\n",
    "        if npatches == 1: good = True\n",
    "    return dict(data=np.array(data_raw), npatches=npatches, good=good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page(doc, page_index, verbose=False):\n",
    "    categories = [\n",
    "        \"Retail & recreation\",\n",
    "        \"Grocery & pharmacy\",\n",
    "        \"Parks\",\n",
    "        \"Transit stations\",\n",
    "        \"Workplace\",\n",
    "        \"Residential\",\n",
    "    ]\n",
    "\n",
    "    counties = []\n",
    "    curr_county = None\n",
    "    curr_category = None\n",
    "    data = defaultdict(lambda: defaultdict(list))\n",
    "    pagetext = doc.getPageText(page_index)\n",
    "    lines = pagetext.splitlines()\n",
    "    tickdates = list(filter(lambda x:len(x.split())==3, set(lines[-10:])))\n",
    "    for line in lines:\n",
    "        # if we encountered a category, add to dict, otherwise\n",
    "        # push all seen lines into the existing dict entry\n",
    "        if any(line.startswith(c) for c in categories):\n",
    "            curr_category = line\n",
    "        elif curr_category:\n",
    "            data[curr_county][curr_category].append(line)\n",
    "\n",
    "        # If it doesn't match anything, then it's a county name\n",
    "        if (all(c not in line for c in categories)\n",
    "            and (\"compared to baseline\" not in line)\n",
    "            and (\"Not enough data\" not in line)\n",
    "           ):\n",
    "            # saw both counties already\n",
    "            if len(data.keys()) == 2: break\n",
    "            counties.append(line)\n",
    "            curr_county = line\n",
    "\n",
    "    newdata = {}\n",
    "    for county in data:\n",
    "        newdata[county] = {}\n",
    "        for category in data[county]:\n",
    "            # if the category text ends with a space, then there was a star/asterisk there\n",
    "            # indicating lack of data. we skip these.\n",
    "            if category.endswith(\" \"): continue\n",
    "            temp = [x for x in data[county][category] if \"compared to baseline\" in x]\n",
    "            if not temp: continue\n",
    "            percent = int(temp[0].split()[0].replace(\"%\",\"\"))\n",
    "            newdata[county][category.strip()] = percent\n",
    "    data = newdata\n",
    "\n",
    "    tomatch = []\n",
    "    for county in counties:\n",
    "        for category in categories:\n",
    "            if category in data[county]:\n",
    "                tomatch.append([county,category,data[county][category]])\n",
    "\n",
    "    if verbose:\n",
    "        print(len(tomatch))\n",
    "        print(data)\n",
    "    \n",
    "    goodplots = []\n",
    "    xrefs = sorted(doc.getPageXObjectList(page_index), key=lambda x:int(x[1].replace(\"X\",\"\")))\n",
    "    for i, xref in enumerate(xrefs):\n",
    "        stream = doc.xrefStream(xref[0]).decode()\n",
    "        info = parse_stream(stream)\n",
    "        if not info[\"good\"]: continue\n",
    "        goodplots.append(info)\n",
    "    if verbose:\n",
    "        print(len(goodplots))\n",
    "    \n",
    "    ret = []\n",
    "    \n",
    "    if len(tomatch) != len(goodplots):\n",
    "        return ret\n",
    "    \n",
    "    \n",
    "    for m,g in zip(tomatch,goodplots):\n",
    "        xs = g[\"data\"][:,0]\n",
    "        ys = g[\"data\"][:,1]\n",
    "        maxys = ys[np.where(xs==xs.max())[0]]\n",
    "        maxy = maxys[np.argmax(np.abs(maxys))]\n",
    "        \n",
    "        \n",
    "        # parsed the tick date labels as text. find the min/max (first/last)\n",
    "        # and make evenly spaced dates, one per day, to assign to x values between\n",
    "        # 0 and 200 (the width of the plots).\n",
    "        ts = list(map(lambda x: pd.Timestamp(x.split(None,1)[-1] + \", 2020\"), tickdates))\n",
    "        low, high = min(ts), max(ts)\n",
    "        dr = list(map(lambda x:str(x).split()[0], pd.date_range(low, high, freq=\"D\")))\n",
    "        lutpairs = list(zip(np.linspace(0,200,len(dr)),dr))\n",
    "\n",
    "        dates = []\n",
    "        values = []\n",
    "        asort = xs.argsort()\n",
    "        xs = xs[asort]\n",
    "        ys = ys[asort]\n",
    "        for x, y in zip(xs,ys):\n",
    "            date = min(lutpairs, key=lambda v:abs(v[0]-x))[1]\n",
    "            dates.append(date)\n",
    "            values.append(round(y,3))\n",
    "\n",
    "        ret.append(dict(\n",
    "            county=m[0],category=m[1],change=m[2],\n",
    "            values=values,\n",
    "            dates=dates,\n",
    "            changecalc=maxy,\n",
    "        ))\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page_country(country, doc, page_index, verbose=False):\n",
    "    categories = [\n",
    "        \"Retail & recreation\",\n",
    "        \"Grocery & pharmacy\",\n",
    "        \"Parks\",\n",
    "        \"Transit stations\",\n",
    "        \"Workplace\",\n",
    "        \"Residential\",\n",
    "        \"Workplaces\"\n",
    "    ]\n",
    "\n",
    "    current_category = None\n",
    "    data = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    # goes through all the lines in the page\n",
    "    # adds them to the list of the last category identified\n",
    "    pagetext = doc.getPageText(page_index)\n",
    "    lines = pagetext.splitlines()\n",
    "\n",
    "    lines_to_search = []\n",
    "    for line in lines:\n",
    "        if \"Sun\" in line:\n",
    "            lines_to_search.append(line)\n",
    "\n",
    "    tick_dates = list(filter(lambda x:len(x.split()) == 3, set(lines_to_search[-3:])))\n",
    "\n",
    "    for line in range(len(lines)):\n",
    "        # if we encountered a category, add to dict, otherwise\n",
    "        # push all seen lines into the existing dict entry\n",
    "        for c in categories:\n",
    "            if lines[line].startswith(c):\n",
    "                current_category = lines[line]\n",
    "        if current_category:\n",
    "            if current_category in data:\n",
    "                data[current_category].append(lines[line-1] + \" \" + lines[line])\n",
    "            else:\n",
    "                data[current_category] = [lines[line-1] + \" \" + lines[line]]\n",
    "\n",
    "    newdata = {}\n",
    "\n",
    "\n",
    "    # Eliminates the categories with too little data, in theory\n",
    "    # Doesn't seem to work\n",
    "    for category in data:\n",
    "        # if the category text ends with a space, then there was a star/asterisk there\n",
    "        # indicating lack of data. we skip these.\n",
    "        if not (category.endswith(\" \") or category.endswith(\"*\")):\n",
    "            temp = [x for x in data[category] if \"compared to baseline\" in x and \"Not enough data\" not in x]\n",
    "            if not temp: continue\n",
    "            try:\n",
    "                percent = int(temp[0].split()[0].replace(\"%\",\"\"))\n",
    "\n",
    "                newdata[category.strip()] = percent\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    data = newdata\n",
    "    # print(data)\n",
    "\n",
    "\n",
    "    tomatch = []\n",
    "\n",
    "    # In the national data, category \"Workplace\" is entered \"Workplaces\"\n",
    "    for category in categories:\n",
    "        if category in data:\n",
    "            if category == \"Workplaces\":\n",
    "                tomatch.append([country, \"Workplace\", data[category]])\n",
    "            else:\n",
    "                tomatch.append([country,category,data[category]])\n",
    "\n",
    "    # Some countries have graphs with gaps without an astrix\n",
    "    if country == \"LI\":\n",
    "        for item in tomatch:\n",
    "            if 'Grocery & pharmacy' in item:\n",
    "                tomatch.remove(item)\n",
    "                break\n",
    "\n",
    "    if country == \"LU\":\n",
    "        for item in tomatch:\n",
    "            if 'Residential' in item:\n",
    "                tomatch.remove(item)\n",
    "                break\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print(len(tomatch))\n",
    "        print(data)\n",
    "\n",
    "    # Gathers all the data which fitz deems good\n",
    "    goodplots = []\n",
    "    xrefs = sorted(doc.getPageXObjectList(page_index), key=lambda x:int(x[1].replace(\"X\",\"\")))\n",
    "    for i, xref in enumerate(xrefs):\n",
    "        stream = doc.xrefStream(xref[0]).decode()\n",
    "        info = parse_stream(stream)\n",
    "        if not info[\"good\"]: continue\n",
    "        goodplots.append(info)\n",
    "    if verbose:\n",
    "        print(len(goodplots))\n",
    "    \n",
    "    result = []\n",
    "\n",
    "\n",
    "    # Fails if the the number of good plots doesn't match the number of expected plots\n",
    "    if len(tomatch) != len(goodplots):\n",
    "        print(\"data :\", data)\n",
    "        print(\"tomatch :\", tomatch)\n",
    "        print(\"goodplots :\", goodplots)\n",
    "        print(len(tomatch), len(goodplots))\n",
    "        print(\"failed\")\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    for m,g in zip(tomatch,goodplots):\n",
    "        xs = g[\"data\"][:,0]\n",
    "        ys = g[\"data\"][:,1]\n",
    "        maxys = ys[np.where(xs == xs.max())[0]]\n",
    "        maxy = maxys[np.argmax(np.abs(maxys))]\n",
    "        \n",
    "        \n",
    "        # parsed the tick date labels as text. find the min/max (first/last)\n",
    "        # and make evenly spaced dates, one per day, to assign to x values between\n",
    "        # 0 and 200 (the width of the plots).\n",
    "        ts = list(map(lambda x: pd.Timestamp(x.split(None,1)[-1] + \", 2020\"), tick_dates))\n",
    "        low, high = min(ts), max(ts)\n",
    "        dr = list(map(lambda x:str(x).split()[0], pd.date_range(low, high, freq=\"D\")))\n",
    "        lutpairs = list(zip(np.linspace(0, 200,len(dr)),dr))\n",
    "\n",
    "        dates = []\n",
    "        values = []\n",
    "        asort = xs.argsort()\n",
    "        xs = xs[asort]\n",
    "        ys = ys[asort]\n",
    "        for x, y in zip(xs,ys):\n",
    "            date = min(lutpairs, key=lambda v:abs(v[0]-x))[1]\n",
    "            dates.append(date)\n",
    "            values.append(round(y,3))\n",
    "\n",
    "        result.append(dict(\n",
    "            country=m[0],category=m[1],change=m[2],\n",
    "            values=values,\n",
    "            dates=dates,\n",
    "            changecalc=maxy,\n",
    "        ))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "def parse_state(state):\n",
    "    data = []\n",
    "    if state in [x.split(\"_US_\",1)[1].split(\"_Mobility\",1)[0] for x in glob.glob(\"mobilityData/US/*.pdf\")]:\n",
    "        document = fitz.Document(f\"mobilityDataPDF/US/{date}_US_{state}_Mobility_Report_en.pdf\")\n",
    "        for i in range(2,document.pageCount-1):\n",
    "            for entry in parse_page(document, i):\n",
    "                entry[\"state\"] = state\n",
    "                entry[\"page\"] = i\n",
    "                data.append(entry)\n",
    "        df = pd.DataFrame(data)\n",
    "        return df[[\"state\",\"county\",\"category\",\"change\",\"changecalc\",\"dates\", \"values\",\"page\"]]\n",
    "\n",
    "    else:\n",
    "        document = fitz.Document(f\"mobilityData/{date}_{state}_Mobility_Report_en.pdf\")\n",
    "        if document.pageCount < 4:\n",
    "            return pd.DataFrame(data)\n",
    "        print(state)\n",
    "        for i in range(2,document.pageCount-1):\n",
    "            for entry in parse_page(document, i):\n",
    "                entry[\"country\"] = state\n",
    "                entry[\"page\"] = i\n",
    "                data.append(entry)\n",
    "        df = pd.DataFrame(data)\n",
    "        return df[[\"country\",\"county\",\"category\",\"change\",\"changecalc\",\"dates\", \"values\",\"page\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def parse_name(document):\n",
    "    lines = document.getPageText(0).splitlines()\n",
    "    return \" \".join(lines[1].split()[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "def parse_country(country):\n",
    "    document = fitz.Document(f\"mobilityDataPDF/{date}_{country}_Mobility_Report_en.pdf\")\n",
    "\n",
    "    country_name = parse_name(document)\n",
    "\n",
    "    data = []\n",
    "    for i in range(0, 2):\n",
    "        for entry in parse_page_country(country, document, i):\n",
    "            entry[\"country\"] = country_name\n",
    "            entry[\"page\"] = i\n",
    "            data.append(entry)\n",
    "    df = pd.DataFrame(data)\n",
    "    return df[[\"country\", \"category\", \"change\", \"changecalc\", \"dates\", \"values\", \"page\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=131.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49fa9da7b6fa465db689b101d96b8bf3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "min() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-121-c19929f144c6>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mcountry\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcountries\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 29\u001B[1;33m     \u001B[0mdataFrame\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mparse_country\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcountry\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     30\u001B[0m     \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-120-92bcddfaa6a3>\u001B[0m in \u001B[0;36mparse_country\u001B[1;34m(country)\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m         \u001B[1;32mfor\u001B[0m \u001B[0mentry\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mparse_page_country\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcountry\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdocument\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m             \u001B[0mentry\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"country\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcountry_name\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m             \u001B[0mentry\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"page\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-117-bf6b1d8c8223>\u001B[0m in \u001B[0;36mparse_page_country\u001B[1;34m(country, doc, page_index, verbose)\u001B[0m\n\u001B[0;32m    119\u001B[0m         \u001B[1;31m# 0 and 200 (the width of the plots).\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    120\u001B[0m         \u001B[0mts\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTimestamp\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m\", 2020\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtick_dates\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 121\u001B[1;33m         \u001B[0mlow\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhigh\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mts\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mts\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    122\u001B[0m         \u001B[0mdr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdate_range\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlow\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhigh\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfreq\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"D\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    123\u001B[0m         \u001B[0mlutpairs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mzip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlinspace\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m200\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mdr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: min() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "states = [x.split(\"_US_\",1)[1].split(\"_Mobility\",1)[0] for x in glob.glob(\"mobilityData/US/*.pdf\")]\n",
    "countries = [x[27:29] for x in glob.glob(\"mobilityDataPDF/*.pdf\")]\n",
    "\n",
    "# for state in tqdm(states):\n",
    "#\n",
    "#     dataFrame = parse_state(state)\n",
    "#     data = []\n",
    "#\n",
    "#     for i,row in dataFrame.iterrows():\n",
    "#         # do a little clean up and unstack the dates/values as separate rows\n",
    "#         dorig = dict()\n",
    "#         dorig[\"state\"] = row[\"state\"].replace(\"_\",\" \")\n",
    "#         dorig[\"county\"] = row[\"county\"]\n",
    "#         dorig[\"category\"] = row[\"category\"].replace(\" & \",\"/\").replace(\" \",\"\").lower()\n",
    "#         dorig[\"page\"] = row[\"page\"]\n",
    "#         dorig[\"change\"] = row[\"change\"]\n",
    "#         dorig[\"changecalc\"] = row[\"changecalc\"]\n",
    "#         for x,y in zip(row[\"dates\"],row[\"values\"]):\n",
    "#             d = dorig.copy()\n",
    "#             d[\"date\"] = x\n",
    "#             d[\"value\"] = y\n",
    "#             data.append(d)\n",
    "#     dataFrame = pd.DataFrame(data)\n",
    "#     dataFrame.to_json(\"data/States/\" + state + \".json\", orient=\"records\", indent=2)\n",
    "\n",
    "\n",
    "for country in tqdm(countries):\n",
    "    dataFrame = parse_country(country)\n",
    "    data = []\n",
    "\n",
    "    for i, row in dataFrame.iterrows():\n",
    "        dorig = dict()\n",
    "        dorig[\"country\"] = row[\"country\"].replace(\"_\", \" \")\n",
    "        dorig[\"category\"] = row[\"category\"].replace(\" & \",\"/\").replace(\" \",\"\").lower()\n",
    "        dorig[\"page\"] = row[\"page\"]\n",
    "        dorig[\"change\"] = row[\"change\"]\n",
    "        dorig[\"changecalc\"] = row[\"changecalc\"]\n",
    "        for date, value in zip(row[\"dates\"], row[\"values\"]):\n",
    "            d = dorig.copy()\n",
    "            d[\"date\"] = date\n",
    "            d[\"value\"] = value\n",
    "            data.append(d)\n",
    "\n",
    "    dataFrame = parse_state(country)\n",
    "    for i,row in dataFrame.iterrows():\n",
    "        # do a little clean up and unstack the dates/values as separate rows\n",
    "        dorig = dict()\n",
    "        dorig[\"country\"] = row[\"country\"].replace(\"_\",\" \")\n",
    "        dorig[\"region\"] = row[\"county\"]\n",
    "        dorig[\"category\"] = row[\"category\"].replace(\" & \",\"/\").replace(\" \",\"\").lower()\n",
    "        dorig[\"page\"] = row[\"page\"]\n",
    "        dorig[\"change\"] = row[\"change\"]\n",
    "        dorig[\"changecalc\"] = row[\"changecalc\"]\n",
    "        for x,y in zip(row[\"dates\"],row[\"values\"]):\n",
    "            d = dorig.copy()\n",
    "            d[\"date\"] = x\n",
    "            d[\"value\"] = y\n",
    "            data.append(d)\n",
    "\n",
    "    dataFrame = pd.DataFrame(data)\n",
    "    dataFrame.to_json(\"data/\" + country + \".json\", orient=\"records\", indent=2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}